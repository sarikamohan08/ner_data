import openai
import os
import tiktoken
import json

# Set your OpenAI API key
openai.api_key = "your-api-key"  # Replace with your actual key

# Load the plain text file (with markdown tables inside)
with open("your_file.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# Optional: Preprocess the text (you can customize this)
cleaned_text = raw_text.strip()

# Function to chunk text to fit token limit
def chunk_text(text, model="text-embedding-3-small", max_tokens=8000):
    enc = tiktoken.encoding_for_model(model)
    paragraphs = text.split("\n\n")  # Splitting by paragraph or logical blocks
    chunks = []
    current_chunk = ""

    for para in paragraphs:
        test_chunk = current_chunk + "\n\n" + para if current_chunk else para
        if len(enc.encode(test_chunk)) <= max_tokens:
            current_chunk = test_chunk
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# Split the text into chunks
chunks = chunk_text(cleaned_text)

# Generate embeddings for each chunk
embeddings = []
for i, chunk in enumerate(chunks):
    print(f"Embedding chunk {i + 1} of {len(chunks)}...")
    response = openai.embeddings.create(
        model="text-embedding-3-small",
        input=chunk
    )
    embeddings.append({
        "text": chunk,
        "embedding": response.data[0].embedding
    })

# Save embeddings to JSON
with open("text_embeddings.json", "w", encoding="utf-8") as f:
    json.dump(embeddings, f)

print("âœ… Embeddings saved to text_embeddings.json")
