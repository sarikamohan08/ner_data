# Multithreaded PDF -> LLM extraction (no cost/token logic).
# Drop this into your file (inside SimplePDFExtractor class or as a sibling function).
import time
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

_thread_local = threading.local()

def _make_thread_local_client(api_key: str, model_name: str):
    """
    Create/reuse an OpenAI client for each thread.
    Adjust to match how you initialize the client in __init__.
    """
    if getattr(_thread_local, "client", None) is None:
        # initialize exactly like you do in SimplePDFExtractor.__init__
        from openai import OpenAI
        import httpx
        base_url = f"https://gateway.ai-npe.humana.com/openai/deployments/{model_name}"
        _thread_local.client = OpenAI(api_key=api_key, base_url=base_url, http_client=httpx.Client(verify=False))
    return _thread_local.client

def extract_pdf_to_text_file_parallel_no_cost(self, pdf_path: str, output_file: str, save_json: str = "ocr_results_parallel.json", max_workers: int = 6):
    """
    Parallel extraction using ThreadPoolExecutor. No cost/token calculations.
    - self: instance of SimplePDFExtractor (expects attributes: api_key, model)
           and method: pdf_to_images(pdf_path, dpi=200) -> list of base64 strings.
    - pdf_path: input PDF file path
    - output_file: path to save final extracted text
    - save_json: path to save per-page timing & status JSON summary
    - max_workers: number of concurrent worker threads to run
    """

    print(f"Starting parallel extraction for: {pdf_path}")
    images_b64 = self.pdf_to_images(pdf_path)   # kept as your existing method: list of base64 strings
    total_pages = len(images_b64)
    print(f"Found {total_pages} pages. Using max_workers={max_workers}...")

    # stable prompt (same as your original prompt)
    prompt_text = """Extract ALL text and tables from this PDF page image.

For regular text:
- Preserve paragraphs and line breaks
- Maintain headings and subheadings
- Keep bullet points and numbered lists

For tables:
- Present the table in clear, structured format using Markdown.
- Preserve the original row and column headers as accurately as possible.
- Label the tables sequentially (e.g., Table 1, Table 2).
- Format tables clearly with columns aligned
- Use | to separate columns
- Include table headers
- Add a blank line before and after each table

If any table appears to be split across pages, merge the parts into a complete table.
Output only the extracted text and tables. Do not add any commentary or explanations.
""".strip()

    # Prebuild per-page messages so worker does not recreate prompt
    page_tasks = []
    for i, img_b64 in enumerate(images_b64, start=1):
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_text},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}", "detail": "high"}}
                ]
            }
        ]
        page_tasks.append((i, messages))

    # Worker function executed inside thread
    def worker(task):
        page_num, messages = task
        client = _make_thread_local_client(self.api_key, self.model)
        start = time.perf_counter()
        try:
            resp = client.chat.completions.create(
                model=self.model,
                messages=messages,
                extra_headers={"api-key": self.api_key, "ai-gateway-version": "v2"},
                max_tokens=4000,
                temperature=0
            )
            text_out = resp.choices[0].message.content.strip()
            error = None
        except Exception as e:
            text_out = f"Error extracting page {page_num}: {e}"
            error = str(e)
        elapsed = time.perf_counter() - start
        return {
            "page": page_num,
            "time_s": elapsed,
            "response_text": text_out,
            "error": error
        }

    # Run the pool
    results = []
    print(f"Submitting {len(page_tasks)} tasks to ThreadPoolExecutor...")
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        future_to_page = {ex.submit(worker, task): task[0] for task in page_tasks}
        for fut in as_completed(future_to_page):
            pg = future_to_page[fut]
            try:
                res = fut.result()
            except Exception as e:
                res = {"page": pg, "time_s": 0.0, "response_text": "", "error": str(e)}
            print(f"Page {res['page']} finished in {res['time_s']:.2f}s, error={res['error'] or 'None'}")
            results.append(res)

    # Sort results by page number so final file preserves original order
    results.sort(key=lambda x: x["page"])

    # Build output text similar to your original flow
    all_text = []
    all_text.append(f"Text extracted from: {os.path.basename(pdf_path)}")
    all_text.append(f"Extraction date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    all_text.append("-" * 60)
    all_text.append("")

    total_model_time = 0.0
    for r in results:
        all_text.append(f"--- PAGE {r['page']} ---")
        all_text.append(f"[Model call time: {r['time_s']:.2f}s] [Error: {r['error'] or 'None'}]")
        all_text.append("")
        all_text.append(r["response_text"])
        all_text.append("")
        total_model_time += (r["time_s"] or 0.0)

    # Save output text
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("\n".join(all_text))
        print(f"Saved extracted text to: {output_file}")
    except Exception as e:
        print("Error saving output text file:", e)

    # Save JSON summary
    summary = {
        "input_file": pdf_path,
        "pages": len(results),
        "total_model_time_s": total_model_time,
        "per_page": [{"page": r["page"], "model_time_s": r["time_s"], "error": r["error"]} for r in results]
    }
    try:
        with open(save_json, "w", encoding="utf-8") as jf:
            json.dump(summary, jf, indent=2)
        print(f"Saved JSON summary to: {save_json}")
    except Exception as e:
        print("Error saving JSON summary:", e)

    print("\n=== FINAL SUMMARY ===")
    print(f"Pages: {len(results)}; Total model time (sum): {total_model_time:.2f}s")
    return summary

# Usage (if method added inside the class):
# extractor = SimplePDFExtractor(api_key="YOUR_KEY")
# extractor.extract_pdf_to_text_file_parallel_no_cost("/path/to/exhibit-1.pdf", "output_text.txt", save_json="summary.json", max_workers=6)


