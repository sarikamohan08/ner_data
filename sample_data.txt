

# One cell to run in Databricks / local Python
# (Single-line pip install as preferred)
import sys
try:
    import tiktoken
except Exception:
    print("tiktoken not found â€” installing...")
    # single-line install (no backslashes)
    !pip install -q tiktoken
    import importlib
    importlib.invalidate_caches()
    import tiktoken

from typing import Tuple

def count_tokens(text: str, model: str = "gpt-4o-mini") -> int:
    """
    Return token count for `text` using tiktoken encoding for `model`.
    If model encoding is not available, falls back to a sensible encoding.
    """
    try:
        enc = tiktoken.encoding_for_model(model)
    except Exception:
        # fallback to cl100k_base (used by many recent OpenAI models)
        try:
            enc = tiktoken.get_encoding("cl100k_base")
        except Exception:
            enc = tiktoken.get_encoding("p50k_base")  # last resort
    tokens = enc.encode(text)
    return len(tokens)

def cost_from_tokens(token_count: int, cost_per_1k_tokens: float) -> float:
    """
    cost_per_1k_tokens: USD cost per 1000 tokens (e.g. 0.003 for $0.003 / 1k tokens)
    """
    return (token_count / 1000.0) * cost_per_1k_tokens

def token_and_cost_report(
    text: str,
    model: str = "gpt-4o-mini",
    cost_per_1k_tokens_prompt: float = None,
    cost_per_1k_tokens_response: float = None,
    assume_response_tokens: int = None
) -> dict:
    """
    Returns a dict with token counts and estimated costs.

    - text: the prompt / input text you will send
    - model: model name to pick encoding
    - cost_per_1k_tokens_prompt: USD per 1000 tokens for prompt (None to skip cost)
    - cost_per_1k_tokens_response: USD per 1000 tokens for response (if model bills differently)
    - assume_response_tokens: if you want to estimate response cost, provide expected response token count
    """
    prompt_tokens = count_tokens(text, model=model)

    report = {
        "model": model,
        "prompt_tokens": prompt_tokens,
        "prompt_cost_usd": None,
        "response_tokens_estimate": None,
        "response_cost_usd": None,
        "total_cost_usd": None,
    }

    if cost_per_1k_tokens_prompt is not None:
        report["prompt_cost_usd"] = cost_from_tokens(prompt_tokens, cost_per_1k_tokens_prompt)

    if assume_response_tokens is not None:
        report["response_tokens_estimate"] = assume_response_tokens
        if cost_per_1k_tokens_response is None:
            # if response price not provided, assume same as prompt
            cost_per_1k_tokens_response = cost_per_1k_tokens_prompt
        if cost_per_1k_tokens_response is not None:
            report["response_cost_usd"] = cost_from_tokens(assume_response_tokens, cost_per_1k_tokens_response)

    # total cost (if available)
    total = 0.0
    if report["prompt_cost_usd"] is not None:
        total += report["prompt_cost_usd"]
    if report["response_cost_usd"] is not None:
        total += report["response_cost_usd"]
    report["total_cost_usd"] = total if total > 0 else None

    return report

# --------------------
# Example usage:
# Replace text_here with your OCR text or any string.
text_here = """Contractor Requirements
CONTRACTOR REQUIREMENTS DOCUMENT
Exhibit B
Contractor Data Security Certificate
... (paste OCR text here) ...
"""

# Example model and costs (replace with official/latest model pricing)
# NOTE: update these numbers to the exact official price per 1k tokens for your model/provider.
model_name = "gpt-4o-mini"
sample_prompt_price_per_1k = 0.003  # example: $0.003 per 1k prompt tokens (change to real price)
sample_response_price_per_1k = 0.003  # example: same for responses

# If you want to estimate the model's reply length, set assume_response_tokens
assumed_response_tokens = 200  # set a guess for response length (or None)

report = token_and_cost_report(
    text_here,
    model=model_name,
    cost_per_1k_tokens_prompt=sample_prompt_price_per_1k,
    cost_per_1k_tokens_response=sample_response_price_per_1k,
    assume_response_tokens=assumed_response_tokens
)

import json
print(json.dumps(report, indent=2))
